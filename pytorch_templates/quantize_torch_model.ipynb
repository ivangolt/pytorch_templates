{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, # type: ignore\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, # type: ignore\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)  # Initialized here\n",
    "\n",
    "        self.fc1 = nn.Linear(7*7*64, 512)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool(x)  \n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool(x) \n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MnistModel()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "MnistModel                               [64, 1, 28, 28]           [64, 10]                  --                        True\n",
       "├─Conv2d: 1-1                            [64, 1, 28, 28]           [64, 32, 28, 28]          320                       True\n",
       "├─BatchNorm2d: 1-2                       [64, 32, 28, 28]          [64, 32, 28, 28]          64                        True\n",
       "├─ReLU: 1-3                              [64, 32, 28, 28]          [64, 32, 28, 28]          --                        --\n",
       "├─MaxPool2d: 1-4                         [64, 32, 28, 28]          [64, 32, 14, 14]          --                        --\n",
       "├─Conv2d: 1-5                            [64, 32, 14, 14]          [64, 64, 14, 14]          18,496                    True\n",
       "├─BatchNorm2d: 1-6                       [64, 64, 14, 14]          [64, 64, 14, 14]          128                       True\n",
       "├─ReLU: 1-7                              [64, 64, 14, 14]          [64, 64, 14, 14]          --                        --\n",
       "├─MaxPool2d: 1-8                         [64, 64, 14, 14]          [64, 64, 7, 7]            --                        --\n",
       "├─Linear: 1-9                            [64, 3136]                [64, 512]                 1,606,144                 True\n",
       "├─ReLU: 1-10                             [64, 512]                 [64, 512]                 --                        --\n",
       "├─Linear: 1-11                           [64, 512]                 [64, 10]                  5,130                     True\n",
       "============================================================================================================================================\n",
       "Total params: 1,630,282\n",
       "Trainable params: 1,630,282\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 351.20\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 38.80\n",
       "Params size (MB): 6.52\n",
       "Estimated Total Size (MB): 45.52\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchinfo\n",
    "torchinfo.summary(model=model, input_size=(64, 1, 28, 28), verbose=0, col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1/5:   0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1/5: 100%|██████████| 938/938 [01:03<00:00, 14.82it/s, Accuracy=96.3, Loss=0.273]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.52% and Loss: 0.04262173857902671\n",
      "Best validation loss: 0.04262173857902671\n",
      "Saving best model for epoch: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2/5: 100%|██████████| 938/938 [01:13<00:00, 12.84it/s, Accuracy=98.578, Loss=0.0097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.49% and Loss: 0.04349938898966377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3/5: 100%|██████████| 938/938 [01:11<00:00, 13.07it/s, Accuracy=98.98, Loss=0.0058] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.06% and Loss: 0.02821018383197535\n",
      "Best validation loss: 0.02821018383197535\n",
      "Saving best model for epoch: 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 4/5: 100%|██████████| 938/938 [01:13<00:00, 12.80it/s, Accuracy=99.19, Loss=0.0473] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 98.96% and Loss: 0.034773673059488405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 5/5: 100%|██████████| 938/938 [01:13<00:00, 12.72it/s, Accuracy=99.39, Loss=0.1013] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.14% and Loss: 0.029976082717447894\n"
     ]
    }
   ],
   "source": [
    "from train_helpers import ClassifierTrainer, save_plots\n",
    "\n",
    "trainer = ClassifierTrainer(\n",
    "    model= model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=trainloader,\n",
    "    val_loader=testloader,\n",
    "    num_epochs=5,\n",
    "    cuda=False\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model (recomendation from docs)\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), '../models/unquantize_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic quantization quantizes the model weights and is carried out dynamically during runtime. The activations are stored in their original floating-point format.\n",
    "\n",
    "Since weights are quantized dynamically at runtime, it allows for more flexibility. It can be beneficial in handling cases where the range of values can vary. As activations remain in their original format, the accuracy loss is usually less than static quantization. Dynamic quantization doesn’t need calibration data, making it simpler to apply.\n",
    "\n",
    "However, as dynamic quantization only quantizes the weights, not the activations, it provides less compression and speedup than static quantization. Since weights are quantized on-the-fly during inference, it may introduce some runtime overhead.\n",
    "\n",
    "Dynamic Quantization is pretty straightforward and requires only a single step for quantization. Let’s load our Unquantized model’s weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize model \n",
    "import torch.quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model (recomendation from docs)\n",
    "torch.save(quantized_model.state_dict(), '../models/dyn_quantized_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_model_size(path:str):\n",
    "    size = os.path.getsize(path)\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size dynamic quantize model: 1657.39 MB\n",
      "Size of non quantize model: 6374.91 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size dynamic quantize model: {get_model_size(path='../models/dyn_quantized_model.pth')/ 1024:.2f} MB\")\n",
    "print(f\"Size of non quantize model: {get_model_size(path='../models/unquantize_model.pth') / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): DynamicQuantizedLinear(in_features=3136, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (fc2): DynamicQuantizedLinear(in_features=512, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model Quantized Model Size(Mb): 1.69575\n",
      "Model Unquantized Model Size(Mb): 6.526686\n",
      "The Quantized Model is smaller by 74.02%.\n",
      "==================================================\n",
      "Accuracy of Quantized Model: 99.15\n",
      "Accuracy of Unquantized Model: 99.14\n",
      "==================================================\n",
      "Average inference time of Quantized Model over 2 iterations: 3.447869658470154\n",
      "Average inference time of Unquantized Model over 2 iterations: 3.4155484437942505\n",
      "The Unquantized Model is faster by 0.94%.\n"
     ]
    }
   ],
   "source": [
    "from utils import ModelCompare\n",
    "model_compare = ModelCompare(\n",
    "    model1=quantized_model,\n",
    "    model1_info=\"Quantized Model\",\n",
    "    model2=model,\n",
    "    model2_info=\"Unquantized Model\",\n",
    "    cuda=False\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_size()\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_accuracy(dataloder=testloader)\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_inference_time(N=2 , dataloder=testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Set the model to evaluation mode with model.eval(). This is important as certain layers like dropout and batchnorm behave differently during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "unquant_model_copy = copy.deepcopy(model)\n",
    "unquant_model_copy.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Define the list of layers in your model architecture that can be fused together for the purpose of quantization. When performing quantization, certain groups of operations can be replaced by single operations that are equivalent but more computationally efficient. . For example, a convolution followed by a batch normalization, followed by a ReLU operation (Conv -> BatchNorm -> ReLU), can be replaced by a single fused ConvBnReLU operation. We will use torch.quantization.fuse_modules to fuse a list of modules into a single module. This has several advantages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_layers = [['conv1', 'bn1', 'relu1'], ['conv2', 'bn2', 'relu2']]\n",
    "fused_model = torch.quantization.fuse_modules(unquant_model_copy, fused_layers, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Next, we will use the QuantizedModel wrapper class to wrap our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedModel(torch.nn.Module):\n",
    "    def __init__(self, model) -> None:\n",
    "        super().__init__()\n",
    "        self.fp32_model = model\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.fp32_model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "quantized_model = QuantizedModel(model=fused_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of this code is to add quantization and dequantization stubs to the model, which will act as ‘anchors’ to insert the actual quantization and dequantization functions in the model graph during the quantization process. The quant_layer converts the numbers in fp32 to int8 so that conv and relu will run in int8 format and then the dequant_layer will perform the int8 to fp32 conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Set the configuration for quantization using the get_default_qconfig function from torch.quantization. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    }
   ],
   "source": [
    "# Select quantization schemes from \n",
    "# https://pytorch.org/docs/stable/quantization-support.html\n",
    "\n",
    "quantized_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "quantized_model.qconfig = quantized_config\n",
    "\n",
    "# Print quantization configurations\n",
    "print(quantized_model.qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“fbgemm” is a high-performance, 8-bit quantization backend that is used on CPUs. It’s currently the recommended backend for quantization when deploying on servers. The qconfig attribute of a PyTorch model is used to specify how the model should be quantized. By assigning quantization_config to quantized_model.qconfig, you’re specifying that the model should be quantized according to the “fbgemm” backend’s default configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Prepare the model for quantization with the torch.quantization.prepare() function. The model is prepared in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedModel(\n",
       "  (fp32_model): MnistModel(\n",
       "    (conv1): ConvReLU2d(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (bn1): Identity()\n",
       "    (relu1): Identity()\n",
       "    (conv2): ConvReLU2d(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (bn2): Identity()\n",
       "    (relu2): Identity()\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(\n",
       "      in_features=3136, out_features=512, bias=True\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (fc2): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.prepare(quantized_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Calibrate the model with the test dataset. Run the model with a few examples to calibrate the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, loader, device=torch.device(\"cpu\")):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        _ = model(inputs)\n",
    "\n",
    "calibrate_model(model=quantized_model, loader=trainloader, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the quantization process, floating-point values are mapped to integer values. For weights, the range is known as they’re static and don’t change post-training. However, activations can vary depending on the input to the network. Calibration, typically performed by passing a subset of the data through the model and collecting the outputs, helps estimate this range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7. Convert the prepared model to a quantized model using torch.quantization.convert(). The conversion is also done in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedModel(\n",
       "  (fp32_model): MnistModel(\n",
       "    (conv1): QuantizedConvReLU2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.047439757734537125, zero_point=0, padding=(1, 1))\n",
       "    (bn1): Identity()\n",
       "    (relu1): Identity()\n",
       "    (conv2): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04675757512450218, zero_point=0, padding=(1, 1))\n",
       "    (bn2): Identity()\n",
       "    (relu2): Identity()\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): QuantizedLinear(in_features=3136, out_features=512, scale=0.6587530970573425, zero_point=77, qscheme=torch.per_channel_affine)\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (fc2): QuantizedLinear(in_features=512, out_features=10, scale=0.5372301340103149, zero_point=66, qscheme=torch.per_channel_affine)\n",
       "  )\n",
       "  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model Quantized Model Size(Mb): 1.64939\n",
      "Model Uquantize model Size(Mb): 6.526686\n",
      "The Quantized Model is smaller by 74.73%.\n",
      "==================================================\n",
      "Accuracy of Quantized Model: 99.16\n",
      "Accuracy of Uquantize model: 99.14\n",
      "==================================================\n",
      "Average inference time of Quantized Model over 2 iterations: 2.316429376602173\n",
      "Average inference time of Uquantize model over 2 iterations: 2.6330199241638184\n",
      "The Quantized Model is faster by 12.02%.\n"
     ]
    }
   ],
   "source": [
    "from utils import ModelCompare\n",
    "model_compare = ModelCompare(\n",
    "    model1=quantized_model,\n",
    "    model1_info=\"Quantized Model\",\n",
    "    model2=model,\n",
    "    model2_info=\"Uquantize model\",\n",
    "    cuda=False\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_size()\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_accuracy(dataloder=testloader)\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_inference_time(N=2 , dataloder=testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static quantization enables the generation of highly efficient quantized integer models for inference. However, despite careful post-training calibration, there may be instances where the model’s accuracy is compromised to an unacceptable extent. In such cases, post-training calibration alone is insufficient for generating a quantized integer model. To account for the quantization effect, the model needs to be trained in a manner that considers quantization. Quantization-aware training addresses this by incorporating fake quantization modules, which simulate the clamping and rounding effects of integer quantization at the specific points where quantization occurs during the conversion from floating-point to quantized integer models. These fake quantization modules also monitor the scales and zero points of the weights and activations. Once the quantization awareness training is completed, the floating-point model can be readily converted to a quantized integer model using the information stored in the fake quantization modules.\n",
    "\n",
    "The Quantization Aware training process borrows similar steps from static quantizaion. Let’s load our Unquantized model’s weight:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the torch state \n",
    "state = torch.load(\"outputs/best_model.pth\")\n",
    "quant_network = MnistModel()\n",
    "\n",
    "# loading the state dict\n",
    "quant_network.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (relu3): ReLU(inplace=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Check the layers that can be fused and fuse the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the layers that can be fused.\n",
    "fused_layers = [['conv1', 'bn1', 'relu1'], ['conv2', 'bn2', 'relu2']]\n",
    "\n",
    "# Fuse the layers\n",
    "torch.quantization.fuse_modules(quant_network, fused_layers, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Wrap the fused model using QuantizedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model_fp32 = model\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# Apply torch.quantization.QuantStub() and torch.quantization.QuantStub() to the inputs and outputs, respectively.\n",
    "quant_network = QuantizedModel(quant_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Set the configuration for quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select quantization schemes from \n",
    "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    " \n",
    "quant_network.qconfig = quantization_config\n",
    "\n",
    "# Print quantization configurations\n",
    "print(quant_network.qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Prepare model for QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedModel(\n",
       "  (model_fp32): MnistModel(\n",
       "    (conv1): Conv2d(\n",
       "      1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (bn1): BatchNorm2d(\n",
       "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (conv2): Conv2d(\n",
       "      32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (bn2): BatchNorm2d(\n",
       "      64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (fc1): Linear(\n",
       "      in_features=3136, out_features=512, bias=True\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (fc2): Linear(\n",
       "      in_features=512, out_features=10, bias=True\n",
       "      (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.quantization.prepare_qat(quant_network, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Train QAT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 1/4: 100%|██████████| 938/938 [01:38<00:00,  9.49it/s, Accuracy=99.495, Loss=0.001] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.11% and Loss: 0.02844548009582636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 2/4: 100%|██████████| 938/938 [01:38<00:00,  9.48it/s, Accuracy=99.502, Loss=0.0212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.05% and Loss: 0.028112857355681278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 3/4: 100%|██████████| 938/938 [01:37<00:00,  9.60it/s, Accuracy=99.502, Loss=0.0002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.07% and Loss: 0.028242666940514027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 4/4: 100%|██████████| 938/938 [01:39<00:00,  9.45it/s, Accuracy=99.487, Loss=0.0046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.05% and Loss: 0.0281196557682897\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = ClassifierTrainer(\n",
    "    model= quant_network,\n",
    "    optimizer=optimizer, \n",
    "    criterion=criterion,\n",
    "    train_loader=trainloader,\n",
    "    val_loader=testloader,\n",
    "    cuda=False,\n",
    "    num_epochs=4\n",
    ")\n",
    "\n",
    "trainer.train(save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Model Quantized Model Size(Mb): 1.64939\n",
      "Model UnQuantized Model Size(Mb): 6.526686\n",
      "The Quantized Model is smaller by 74.73%.\n",
      "==================================================\n",
      "Accuracy of Quantized Model: 99.16\n",
      "Accuracy of UnQuantized Model: 99.14\n",
      "==================================================\n",
      "Average inference time of Quantized Model over 10 iterations: 2.267369604110718\n",
      "Average inference time of UnQuantized Model over 10 iterations: 3.1072409629821776\n",
      "The Quantized Model is faster by 27.03%.\n"
     ]
    }
   ],
   "source": [
    "from utils import ModelCompare\n",
    "model_compare = ModelCompare(\n",
    "    model1=quantized_model,\n",
    "    model1_info=\"Quantized Model\",\n",
    "    model2=model,\n",
    "    model2_info=\"UnQuantized Model\",\n",
    "    cuda=False)\n",
    "\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_size()\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_accuracy(dataloder=testloader)\n",
    "print(\"=\"*50)\n",
    "model_compare.compare_inference_time(N=10 , dataloder=testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
