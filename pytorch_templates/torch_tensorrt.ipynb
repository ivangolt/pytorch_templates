{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT is a machine learning framework that is published by Nvidia to run inference that is machine learning inference on their hardware. TensorRT is highly optimized to run on NVIDIA GPUs. It's likely the fastest way to run a model at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TensorRT** — фреймворк, который разрабатывается компанией Nvidia, а значит, позволяет разработчикам учесть нюансы аппаратного устройства вычислителей.\n",
    "> \n",
    "\n",
    "В нём модель оптимизируется перед запуском таким образом, чтобы для вычисления каждой операции использовались самые быстрые алгоритмы на конкретном железе с определённым количеством свободной памяти (именно в целевом приложении).\n",
    "\n",
    "\n",
    "\n",
    "Перед отправкой модели в TensorRT нужно конвертировать её в ONNX — это удобный способ передачи модели между фреймворками. TensorRT поддерживает и динамическое построение модели через создание **NetworkDefinition**. Некоторые фреймворки (например, [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt)) используют такую функцию для вычисления в TensorRT частей модели (другие вычисляются на PyTorch). Но обычно это медленнее и связано с наличием у модели операций, которые не поддерживаются TensorRT или ONNX.\n",
    "\n",
    "Полезная вещь для ONNX-модели — применение оптимизаций из **onnx-simplifier** или **onnx-graphsurgeon**. PyTorch конвертирует модель в ONNX операция за операцией, а эти библиотеки распознают их паттерны, заменяют на более простые аналоги, не нарушая семантику вычислительного графа. Пример оптимизации с помощью onnx-simplifier представлен на картинке ниже. Перед запуском TensorRT для упрощения ему работы рекомендуется применить onnx-simplifier к вашему целевому onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch_tensorrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.ResNet50_Weights.DEFAULT\n",
    "model = torchvision.models.resnet50(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model.eval().cuda()\n",
    "\n",
    "# define input\n",
    "x = torch.randn((1, 3, 224, 224)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt.dynamo.utils:Using Default Torch-TRT Runtime (as requested by user)\n",
      "INFO:torch_tensorrt.dynamo.utils:Device not specified, using Torch default current device - cuda:0. If this is incorrect, please specify an input device, via the device keyword.\n",
      "INFO:torch_tensorrt.dynamo.utils:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f32: 7>}, debug=False, workspace_size=0, min_block_size=5, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)\n",
      "\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Node _param_constant0 of op type get_attr does not have metadata. This could sometimes lead to undefined behavior.\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 7305, GPU 1401 (MiB)\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +75, GPU +288, now: CPU 7380, GPU 1689 (MiB)\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:03.871509\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Detected 1 inputs and 1 output network tensors.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Host Persistent Memory: 355792\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Device Persistent Memory: 1536\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Scratch Memory: 4608\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Started assigning block shifts. This will take 87 steps to complete.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Algorithm ShiftNTopDown took 2.8677ms to assign 5 blocks to 87 nodes requiring 7230464 bytes.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Activation Memory: 7229952\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Weights Memory: 112703904\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Engine generation completed in 18.073 seconds.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 238 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 2478 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 26 bytes of code generator cache.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 324 timing cache entries\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:18.247343\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 116132468 bytes of Memory\n",
      "INFO:torch_tensorrt.dynamo.conversion._conversion:Since Torch-TensorRT runtime is not available, using Python Runtime, some features may not be available\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Loaded engine size: 110 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +7, now: CPU 1, GPU 228 (MiB)\n",
      "WARNING:torch_tensorrt [TensorRT Conversion Context]:Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-5.0904e-01,  2.1708e-01,  4.9388e-01,  1.1321e-01,  1.0922e-01,\n",
       "         -8.5650e-02, -6.7061e-01, -3.3510e-01, -3.3576e-02,  5.3123e-01,\n",
       "          2.1692e+00,  2.3855e+00,  3.1600e+00,  1.2145e+00,  2.3697e+00,\n",
       "          2.5355e+00,  3.1621e+00,  7.3838e-01,  3.5306e+00,  1.5613e+00,\n",
       "          9.3568e-01,  5.1983e+00,  4.6880e+00,  4.2450e+00,  1.5544e+00,\n",
       "         -6.2847e-01, -5.2865e-01, -4.0366e-01, -4.0650e-01, -4.0630e-01,\n",
       "         -6.0638e-01,  3.1652e-01, -6.6534e-01, -7.3297e-01, -4.6542e-01,\n",
       "         -8.4165e-01, -2.9450e-01, -9.5617e-01, -2.2987e-01, -3.5871e-01,\n",
       "          3.1342e-01, -6.5971e-01,  2.4042e-01,  4.4494e-02, -4.0216e-01,\n",
       "         -4.8257e-01,  2.1305e-01,  1.0862e-01, -6.0703e-01, -5.7100e-01,\n",
       "         -2.5311e-02, -6.9373e-01, -3.2193e-01, -5.9247e-01, -2.5352e-01,\n",
       "          2.5007e-01, -2.9238e-01, -5.1614e-01, -4.1690e-01,  1.7109e+00,\n",
       "          8.7341e-02, -8.7084e-01, -4.8313e-01,  1.6086e-01,  5.5343e-01,\n",
       "         -5.9889e-01, -6.4109e-01, -3.0300e-01, -1.7307e-01,  2.4521e-01,\n",
       "          2.2426e-01,  2.4892e-01,  3.7640e-01,  1.0072e+00,  5.0929e-01,\n",
       "          5.0438e-01, -3.9753e-01, -4.7667e-03,  1.1307e+00,  4.7809e-01,\n",
       "          1.2957e+00,  1.1034e+00,  9.2002e-01,  1.3577e-01, -6.9133e-04,\n",
       "          1.8793e+00,  1.2557e+00,  2.2596e-01,  9.9795e-01,  3.3347e+00,\n",
       "          1.1626e+00,  1.7697e+00,  4.6584e+00,  1.5818e+00,  3.2374e+00,\n",
       "          1.0211e+00,  1.9959e+00,  3.9144e-01,  9.8685e-01,  1.4136e+00,\n",
       "          4.3780e-01, -5.9145e-01, -6.9220e-01,  4.6040e-01, -4.4964e-01,\n",
       "         -9.0510e-01, -9.5333e-01,  1.3863e+00, -6.9085e-01, -3.4453e-01,\n",
       "          1.3581e-01,  2.2683e+00,  3.0567e-01,  1.3472e-01, -5.8432e-01,\n",
       "         -6.5591e-01, -2.7684e-01,  1.4940e-01, -1.1995e+00, -5.9047e-01,\n",
       "         -7.8668e-02, -9.4353e-01, -2.0981e-01, -4.7221e-01, -6.5862e-01,\n",
       "         -4.8129e-01,  5.4554e-01,  4.5525e+00,  4.0166e+00,  2.6171e+00,\n",
       "          4.7594e-01,  9.2117e-01,  2.3861e+00,  1.5591e+00,  1.7247e+00,\n",
       "          8.6191e-01, -1.3023e-01,  4.1845e-01,  7.4211e-01,  9.2750e-03,\n",
       "          5.2797e-01,  2.1154e+00,  1.1877e+00,  1.8330e+00,  2.1890e+00,\n",
       "          1.8916e-01,  2.8404e+00, -7.1650e-01, -3.0826e-01,  5.0415e-03,\n",
       "         -7.8787e-01, -1.1653e-01, -7.0523e-01, -1.4680e-01, -9.9293e-01,\n",
       "         -8.4657e-01, -7.9215e-01, -6.8895e-01, -1.2941e+00, -6.0612e-01,\n",
       "         -7.5794e-01, -1.0159e+00, -4.7522e-01,  4.4657e-02, -9.3370e-01,\n",
       "         -1.1141e+00, -5.3038e-01, -7.8002e-01, -7.0931e-01, -1.0836e+00,\n",
       "         -3.7921e-01, -1.1384e-01, -1.0777e-01,  3.5598e-02, -6.2001e-01,\n",
       "         -8.8939e-01, -6.6800e-01, -6.0763e-01, -7.1512e-01, -8.3709e-01,\n",
       "         -4.5814e-01, -6.8797e-01, -3.0960e-01, -1.0656e+00, -9.0184e-01,\n",
       "         -8.8470e-01, -1.1351e+00,  6.8025e-02, -5.7375e-01, -2.4455e-01,\n",
       "         -5.6033e-01, -1.2115e+00, -6.7030e-01, -4.3357e-01, -4.6828e-01,\n",
       "         -7.0304e-01, -9.8026e-01, -1.0904e+00, -8.9680e-01, -5.0401e-01,\n",
       "         -3.5403e-01, -4.5128e-01, -5.7148e-01, -2.0041e-01, -4.3970e-01,\n",
       "         -9.6441e-01, -8.4707e-01, -1.0324e+00, -2.9144e-01, -9.2956e-01,\n",
       "         -1.0078e+00, -5.7127e-01, -6.1239e-01, -7.0174e-01, -9.3410e-01,\n",
       "         -1.0079e+00, -5.4131e-01, -3.6799e-01, -2.0645e-01, -3.4134e-01,\n",
       "         -3.1786e-01, -9.1165e-01, -2.8385e-01, -1.2437e+00, -5.0132e-01,\n",
       "         -5.7685e-01, -8.8808e-01, -8.7734e-01, -7.2975e-01, -7.1076e-01,\n",
       "         -9.4834e-01, -5.2665e-01, -7.8618e-01, -7.2117e-01, -1.0533e+00,\n",
       "         -5.8214e-01, -3.3592e-01, -4.2903e-01, -3.6466e-01, -3.6577e-01,\n",
       "         -4.3486e-01, -7.8199e-01, -5.1223e-01, -1.0338e+00, -6.9341e-01,\n",
       "         -4.5061e-01, -8.7378e-01, -7.5263e-01, -7.6891e-01, -8.3148e-01,\n",
       "         -5.0349e-01, -1.9312e-01, -5.7640e-01, -5.2580e-01, -1.9424e-01,\n",
       "         -7.8292e-01, -7.1682e-01, -9.7823e-01, -4.4196e-01, -4.5814e-01,\n",
       "         -7.3734e-01, -4.0405e-01, -6.2163e-01, -8.6649e-01, -4.0161e-01,\n",
       "         -4.5084e-01, -7.9683e-01, -7.6582e-01, -6.6480e-01, -5.4866e-01,\n",
       "         -5.3306e-01, -8.4153e-01, -1.7097e-01, -3.6178e-01, -9.7579e-01,\n",
       "         -7.6877e-01, -8.0751e-01, -6.1320e-01, -6.2683e-01, -2.3224e-01,\n",
       "         -4.0211e-01, -6.3601e-02, -4.6496e-01, -1.5748e-01, -4.0350e-01,\n",
       "         -1.1365e-01, -5.0354e-01, -9.3149e-01,  6.3133e-01, -1.2832e+00,\n",
       "         -6.5573e-01, -4.9061e-01, -1.0211e+00, -4.7562e-02, -5.2170e-01,\n",
       "         -5.7879e-01, -7.2073e-01, -3.8449e-01, -6.6518e-01, -1.4843e-01,\n",
       "         -3.4443e-01,  7.8259e-01, -4.4783e-01,  5.3785e-01,  4.9175e-01,\n",
       "         -3.1974e-01,  4.4359e-01,  1.3720e+00,  5.4490e-01, -1.4263e-01,\n",
       "          9.2139e-01,  1.4712e-01,  6.2395e-01,  3.7085e-01,  2.8662e-01,\n",
       "          1.0538e+00,  2.1032e-01,  1.4955e+00,  1.3040e+00,  1.7308e+00,\n",
       "          7.8585e-01,  1.1029e-01, -4.2105e-01,  1.0082e+00,  3.9400e-01,\n",
       "         -7.8614e-02,  7.1162e-01, -1.1125e-01, -1.2476e-01, -6.6684e-01,\n",
       "         -2.7725e-01, -1.4301e-03, -6.8564e-01, -7.1600e-01, -4.2397e-01,\n",
       "         -1.0571e-01, -1.4604e-01, -9.2149e-01, -8.9609e-01, -9.2543e-01,\n",
       "         -6.9024e-01, -1.1585e+00, -6.8201e-01, -9.0810e-01, -7.2001e-01,\n",
       "         -9.6788e-01, -9.9967e-01, -6.8470e-01,  4.7890e-01,  2.9232e-01,\n",
       "          7.8237e-01, -4.0315e-01, -5.9874e-01, -1.0367e-01, -1.2317e-01,\n",
       "         -3.7625e-01, -9.1702e-02, -5.6332e-01, -4.1656e-01, -1.5208e-01,\n",
       "         -6.8148e-01, -4.9986e-01, -7.6054e-01, -5.2532e-01,  7.1385e-01,\n",
       "          6.6223e-01, -6.7482e-01, -2.7998e-01,  1.8261e-01, -1.4900e-01,\n",
       "         -4.0310e-01, -1.0271e+00, -6.1679e-01,  1.1647e-01, -3.4262e-01,\n",
       "         -9.4224e-02, -6.9332e-02, -2.6355e-01, -3.3236e-01,  3.3100e-01,\n",
       "         -1.7676e-01,  3.7780e-01, -2.4806e-01, -1.0186e-01, -4.9630e-01,\n",
       "         -7.7861e-01, -1.1026e+00, -6.3151e-01, -4.9039e-01, -6.7125e-01,\n",
       "         -2.7643e-01, -3.7191e-01, -6.7906e-01, -5.4485e-01, -1.2004e+00,\n",
       "         -3.2755e-01, -6.1442e-01,  2.7819e-02, -1.4282e-01,  5.8003e-01,\n",
       "         -6.2990e-01, -9.4032e-02,  4.1701e-01, -1.7746e-01,  1.7058e+00,\n",
       "          3.1327e+00, -1.2116e+00, -5.7187e-01, -9.2985e-01,  1.2545e+00,\n",
       "         -8.2498e-01,  1.1150e+00, -1.6699e-02,  4.9013e-01, -1.8119e-01,\n",
       "         -1.1394e+00,  1.1229e+00,  2.2780e+00,  1.0895e+00,  5.9863e-01,\n",
       "         -1.4800e-01,  1.3457e+00, -9.1354e-01, -5.4623e-01, -8.4482e-01,\n",
       "         -1.0270e-01, -4.8368e-01, -2.9225e-01, -5.3778e-01,  4.2375e-01,\n",
       "         -2.4571e-01, -6.6937e-01,  2.0292e-01, -1.7541e-03, -3.6109e-01,\n",
       "         -5.2020e-01, -3.3174e-01,  1.5565e+00,  9.4375e-02, -6.3838e-01,\n",
       "          1.6489e-01, -5.8775e-01,  1.4274e+00,  3.3841e-01, -5.5067e-01,\n",
       "         -4.3220e-01,  1.7153e+00,  1.3182e-01,  1.0242e+00, -2.6149e-01,\n",
       "         -1.2544e+00,  9.6736e-01, -2.0765e-01, -8.3470e-01, -1.1540e+00,\n",
       "          1.5701e-01,  1.4253e-02,  1.4653e-01,  1.0746e+00,  3.5744e-01,\n",
       "         -5.6462e-02, -2.8997e-01,  9.6320e-01,  5.6502e-01, -1.8095e-01,\n",
       "         -4.8770e-01, -5.1787e-01, -1.2589e+00, -1.5698e-01, -3.1085e-01,\n",
       "          2.6418e-01, -4.9745e-01, -4.5347e-01,  1.9809e+00, -2.3256e-01,\n",
       "         -4.2150e-01, -7.4910e-01, -8.3181e-01, -2.6214e-01, -1.8909e-01,\n",
       "         -2.4554e-01,  4.5406e-01, -6.1860e-01,  2.4891e-02,  1.0036e+00,\n",
       "         -4.3163e-01,  5.8820e-01,  4.7134e-01,  2.6534e+00,  8.4752e-01,\n",
       "          1.3551e+00,  1.5951e-02, -4.7660e-01, -5.2264e-01,  7.9820e-01,\n",
       "         -8.7993e-01, -2.8580e-01,  7.6192e-01,  3.4693e-01,  3.8479e-01,\n",
       "          9.5631e-03,  2.3214e-01, -2.2881e-01,  2.1832e-01, -6.6783e-02,\n",
       "         -2.4157e-01,  7.0403e-01,  1.5403e-01,  1.0356e-01, -9.3736e-01,\n",
       "          6.3000e-01, -8.6814e-01,  1.6132e+00,  6.3650e-01, -8.2648e-02,\n",
       "         -6.7056e-02,  5.1041e-02,  2.9828e+00, -1.2265e-01, -5.5877e-01,\n",
       "         -2.7005e-02, -5.9947e-01, -7.5112e-01,  3.4884e-01, -1.1560e+00,\n",
       "         -9.0964e-01, -5.7850e-01, -1.7770e-01, -1.7307e-01, -2.2536e-01,\n",
       "          1.6747e+00, -2.4444e-01, -9.0424e-01,  1.3641e-01, -4.8311e-02,\n",
       "         -1.1137e+00, -9.6760e-01, -3.7635e-01,  1.6060e+00,  7.9220e-01,\n",
       "         -9.1316e-02,  2.3439e-03,  4.2631e-01,  7.3915e-01, -7.2637e-01,\n",
       "          2.0896e-02,  5.0738e-01, -7.3272e-01, -6.5466e-01,  1.8830e+00,\n",
       "         -6.9993e-01, -1.9338e-01, -1.2148e-01, -5.6338e-01,  1.2915e-01,\n",
       "         -1.0942e+00, -8.1603e-02,  3.2944e+00,  2.9389e-01,  3.4869e-01,\n",
       "         -7.9359e-02, -6.0615e-01, -5.0140e-01,  1.3878e+00, -1.1819e+00,\n",
       "         -7.8725e-01, -7.2785e-01, -5.7343e-01, -6.1958e-01, -6.7313e-01,\n",
       "          1.8732e-01, -5.2432e-01,  2.5727e-02, -8.2888e-01,  1.2970e+00,\n",
       "         -9.0493e-01, -7.4412e-01,  2.0537e-01, -3.6855e-01, -6.1471e-01,\n",
       "          4.9810e-02, -7.2527e-01, -1.2828e+00, -3.4145e-01,  1.0988e+00,\n",
       "         -3.8615e-01, -8.8340e-01,  8.2578e-01, -3.1948e-01,  1.6104e-01,\n",
       "         -7.6168e-01, -4.6019e-02, -7.0886e-02,  2.5817e-01, -4.3951e-01,\n",
       "         -4.5865e-01,  1.2267e-01,  9.0323e-02, -1.8359e-01,  7.3224e-01,\n",
       "          2.4200e+00,  8.9845e-02,  2.3361e-01, -7.0180e-01,  3.9032e-01,\n",
       "          1.7386e-01,  4.0093e-01,  5.7799e-01,  6.2067e-02, -7.1083e-01,\n",
       "         -5.9968e-02, -6.6387e-02, -1.0969e+00,  2.7790e-01, -8.7082e-01,\n",
       "         -2.8881e-01,  1.5396e+00, -3.6177e-01,  1.0314e+00,  1.6154e+00,\n",
       "         -9.5011e-02, -3.7874e-02, -4.2887e-02,  1.1192e+00, -3.7223e-01,\n",
       "         -4.6723e-01,  1.4810e+00, -8.6863e-01,  3.5194e-01,  6.8695e-01,\n",
       "          2.0992e-01,  7.7701e-01,  2.8305e-01,  1.3322e-01, -5.3018e-01,\n",
       "          2.9129e-01,  1.0833e+00, -5.3619e-01,  4.6170e-01, -3.2408e-01,\n",
       "         -4.6830e-01,  5.7874e-01, -5.3417e-01,  5.7389e-01,  2.9353e+00,\n",
       "          2.8237e-01, -4.2331e-01, -3.6967e-01, -3.2474e-01,  3.2481e-01,\n",
       "          1.0762e+00, -7.4550e-01, -5.0683e-01, -8.4816e-01, -9.6949e-01,\n",
       "          6.1660e-01, -8.2230e-01,  9.2554e-01, -1.1175e-01, -1.7125e-01,\n",
       "         -3.5199e-01, -8.9966e-01, -3.5891e-01,  2.2034e-02,  1.3406e-01,\n",
       "         -9.4649e-01, -2.0422e-01,  4.9102e-01,  9.4107e-01, -1.0033e+00,\n",
       "         -1.0175e+00, -4.2383e-01, -4.0052e-01, -1.2235e-01,  2.9056e-01,\n",
       "         -9.3273e-01, -4.2352e-01,  1.2378e+00, -4.8297e-01,  7.1672e-01,\n",
       "          1.5132e-01,  6.5321e-01,  2.2053e+00,  5.4516e-01, -4.2147e-01,\n",
       "         -3.4604e-01, -9.1016e-01, -8.6324e-01, -6.6627e-01, -7.3697e-01,\n",
       "         -5.8067e-01, -1.1772e-02,  3.2237e-01,  5.0901e-02, -4.7013e-01,\n",
       "          8.4255e-02,  1.5820e+00,  9.1063e-02,  3.2130e-01, -1.0307e-01,\n",
       "          4.9158e-01,  3.6066e+00,  1.4577e-01,  2.6559e-01,  8.8741e-03,\n",
       "         -7.9393e-01, -5.6253e-01, -7.0516e-02,  5.5567e-01, -1.4048e-01,\n",
       "          6.1099e-01,  3.2227e-01, -2.7129e-01, -7.8336e-01,  7.9686e-01,\n",
       "         -7.7106e-01,  7.6849e-02, -8.3302e-01,  8.5825e-01,  4.1070e-01,\n",
       "          6.5451e-01,  5.8775e-01,  1.3493e+00,  1.1396e+00,  2.7762e-02,\n",
       "         -1.0291e-01, -2.9967e-01,  3.6449e-01,  7.1398e-01, -5.8885e-01,\n",
       "         -6.0814e-01,  8.8642e-01, -3.0679e-01,  2.4535e+00, -5.7529e-01,\n",
       "          1.3340e+00,  2.1471e-01,  8.5432e-02, -5.6625e-01, -9.5662e-01,\n",
       "          1.4911e-01,  8.4841e-02,  8.9214e-03, -2.0184e-01,  3.3758e-01,\n",
       "         -4.5273e-01, -5.7367e-01, -9.6366e-02, -1.7438e-01,  1.4179e+00,\n",
       "         -5.7746e-01, -9.5661e-01, -6.8143e-02,  1.0948e-01, -5.3557e-01,\n",
       "          9.8100e-01, -6.7710e-01, -8.3581e-01, -7.3299e-01, -4.7832e-01,\n",
       "          4.3771e-01, -3.9381e-02, -6.7259e-01,  8.6245e-01,  6.6065e-01,\n",
       "         -1.0293e-01, -7.4718e-01,  4.4463e-01, -9.1663e-02,  1.9186e+00,\n",
       "         -5.6827e-01, -5.1429e-01,  9.9809e-01,  3.7151e-01, -1.0350e-01,\n",
       "         -2.6483e-01,  1.3541e+00,  9.9441e-01, -4.0973e-01, -3.4332e-02,\n",
       "          1.4404e-01,  4.1044e-01,  1.3606e+00,  1.5199e+00,  9.2976e-01,\n",
       "         -3.3096e-01, -7.5480e-01, -8.2626e-01, -5.7364e-01,  8.3128e-02,\n",
       "         -5.4801e-02, -1.9483e-02, -1.0980e-01,  2.3326e-01,  5.7490e-01,\n",
       "          8.0084e-01, -2.3129e-01, -3.6131e-01,  1.0508e-01, -1.9662e-01,\n",
       "         -1.1486e+00, -1.8352e-01, -1.8799e-01, -2.6862e-01,  1.1295e+00,\n",
       "          3.7776e-01,  1.1349e-01, -2.1616e-01, -2.1390e-01, -7.6696e-01,\n",
       "         -7.7798e-01,  5.2380e-01,  2.5715e+00,  7.3294e-01,  5.3561e-01,\n",
       "         -3.1335e-01, -4.5203e-01, -6.1183e-01,  1.6747e+00, -4.2661e-01,\n",
       "         -5.7070e-01,  3.8887e-01, -5.6395e-01,  1.2197e-01,  1.0678e+00,\n",
       "         -7.1954e-01,  8.8157e-01, -7.1782e-01,  9.5249e-01, -4.2765e-01,\n",
       "         -3.5189e-01, -3.8279e-01,  9.2892e-01, -5.1241e-01,  1.1442e-01,\n",
       "         -8.6733e-01,  7.9352e-02,  1.0698e+00, -6.0973e-01, -9.3204e-02,\n",
       "         -7.6157e-02,  1.7885e-01,  3.6476e-02, -1.1096e-01,  2.0518e+00,\n",
       "          9.4991e-01,  1.5699e-01, -5.3552e-01, -3.6646e-01, -3.5644e-01,\n",
       "          2.7506e-01,  1.4612e+00,  1.6064e+00, -1.1681e-01,  1.4069e-01,\n",
       "          5.0446e-01, -9.2081e-01, -9.4300e-01,  1.9242e+00, -7.8967e-02,\n",
       "         -6.6131e-01,  6.0830e-01,  1.7617e+00,  1.2195e+00, -8.4012e-01,\n",
       "         -7.7397e-01, -9.2075e-01, -2.5645e-01,  3.6834e-01, -4.7587e-01,\n",
       "         -1.4114e-01, -4.1170e-01,  3.4740e-01,  3.6181e-01, -1.0359e+00,\n",
       "          5.4242e-01, -1.5252e-01, -4.6937e-01, -5.7395e-01,  1.6316e+00,\n",
       "         -2.5233e-01, -7.6595e-01,  3.3843e-01, -3.0284e-01, -2.2090e-01,\n",
       "         -5.1999e-01, -7.0685e-01, -1.3822e+00, -3.3691e-01, -1.5036e-01,\n",
       "         -5.4284e-01, -7.2230e-01,  2.8477e+00,  3.8844e-01, -8.9546e-01,\n",
       "          3.1212e+00, -5.4580e-01, -1.2873e-01,  4.4765e-01, -1.3949e-01,\n",
       "          1.6776e+00, -1.1396e+00,  1.2371e+00, -1.5205e-01,  2.9206e+00,\n",
       "          1.5225e+00,  2.6686e-01,  4.1100e-02,  2.1811e+00, -1.0893e+00,\n",
       "          1.5732e+00,  2.4877e-01, -9.6842e-01, -3.7409e-01, -5.4858e-01,\n",
       "         -9.9749e-01,  1.1632e+00, -1.1574e+00, -2.0550e-01,  2.0717e+00,\n",
       "          2.4593e+00,  4.7218e-01,  1.1691e-01, -7.5141e-01, -1.0128e+00,\n",
       "         -7.4814e-01, -1.0286e+00, -8.0319e-01, -2.3494e-01,  4.0171e-01,\n",
       "         -5.3322e-01, -8.9366e-01, -7.4968e-01, -9.0674e-01, -5.6562e-01,\n",
       "         -6.6250e-01, -8.5639e-01, -3.4359e-01, -4.2469e-01, -5.4177e-01,\n",
       "         -5.3602e-01, -5.8755e-01, -3.8380e-01,  1.4360e-02, -5.8249e-01,\n",
       "          4.1129e-01, -1.4660e-01, -6.8321e-01,  2.3477e-01,  7.6650e-01,\n",
       "          2.4816e-01,  4.1965e-01, -4.2535e-01, -3.7596e-03,  6.5413e-01,\n",
       "         -7.9926e-01, -2.8552e-01, -9.8132e-02,  1.9211e-01, -9.1036e-01,\n",
       "         -5.7486e-01, -8.7126e-01, -4.0420e-01, -8.8959e-01, -7.6462e-01,\n",
       "         -8.1714e-01,  1.3213e-01, -7.9402e-02,  1.7101e-01, -6.0423e-01,\n",
       "         -8.0762e-01,  2.6232e+00, -1.2954e-01, -4.3108e-01, -1.0612e-01,\n",
       "         -8.1032e-01, -1.4334e-01,  1.1149e-01, -2.0801e-01, -7.4738e-01,\n",
       "          2.9480e-01, -4.0567e-01, -1.1697e+00,  1.8031e-01,  6.7034e-01,\n",
       "          3.3540e-01, -2.4290e-01, -8.3307e-01,  1.9456e-01,  5.4626e-01,\n",
       "          1.6435e-01, -4.2406e-01, -9.4899e-01, -7.1426e-01, -1.0317e+00,\n",
       "         -1.8828e-01, -3.8350e-01, -7.5772e-01, -3.0504e-02,  5.0732e-01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit\n",
    "optimized_model = torch.compile(model=model, backend='tensorrt')\n",
    "optimized_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.95 ms ± 41.8 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "optimized_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2 ms ± 907 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option2. Export "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Optimize + serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch.randn((1, 3, 224, 224)).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:torch_tensorrt._compile:ir was set to default, using dynamo frontend\n",
      "INFO:torch_tensorrt.dynamo._compiler:Compilation Settings: CompilationSettings(enabled_precisions={<dtype.f32: 7>}, debug=False, workspace_size=0, min_block_size=5, torch_executed_ops=set(), pass_through_build_failures=False, max_aux_streams=None, version_compatible=False, optimization_level=None, use_python_runtime=False, truncate_double=False, use_fast_partitioner=True, enable_experimental_decompositions=False, device=Device(type=DeviceType.GPU, gpu_id=0), require_full_compilation=False, disable_tf32=False, assume_dynamic_shape_support=False, sparse_weights=False, refit=False, engine_capability=<EngineCapability.STANDARD: 1>, num_avg_timing_iters=1, dla_sram_size=1048576, dla_local_dram_size=1073741824, dla_global_dram_size=536870912, dryrun=False, hardware_compatible=False)\n",
      "\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Node max_pool2d_default of op type call_function does not have metadata. This could sometimes lead to undefined behavior.\n",
      "WARNING:torch_tensorrt.dynamo._compiler:Some nodes do not have metadata (shape and dtype information). This could lead to problems sometimes if the graph has PyTorch and TensorRT segments.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 6651, GPU 1521 (MiB)\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] Init builder kernel library: CPU +629, GPU +286, now: CPU 7280, GPU 1807 (MiB)\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT INetwork construction elapsed time: 0:00:02.035593\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Detected 1 inputs and 1 output network tensors.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Host Persistent Memory: 355792\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Device Persistent Memory: 1536\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Scratch Memory: 4608\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Started assigning block shifts. This will take 87 steps to complete.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[BlockAssignment] Algorithm ShiftNTopDown took 3.2594ms to assign 5 blocks to 87 nodes requiring 7230464 bytes.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Activation Memory: 7229952\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Total Weights Memory: 112703904\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Engine generation completed in 15.192 seconds.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 10 MiB, GPU 344 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 2478 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 26 bytes of code generator cache.\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Serialized 323 timing cache entries\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:Build TRT engine elapsed time: 0:00:15.313981\n",
      "INFO:torch_tensorrt.dynamo.conversion._TRTInterpreter:TRT Engine uses: 116007532 bytes of Memory\n",
      "INFO:torch_tensorrt.dynamo.conversion._conversion:Since Torch-TensorRT runtime is not available, using Python Runtime, some features may not be available\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:Loaded engine size: 110 MiB\n",
      "INFO:torch_tensorrt [TensorRT Conversion Context]:[MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +7, now: CPU 1, GPU 343 (MiB)\n",
      "WARNING:torch_tensorrt [TensorRT Conversion Context]:Using default stream in enqueueV3() may lead to performance issues due to additional calls to cudaStreamSynchronize() by TensorRT to ensure correct synchronization. Please use non-default stream instead.\n",
      "WARNING:py.warnings:c:\\VSCode_projects\\pytorch_gpu\\.venv\\Lib\\site-packages\\torch\\jit\\_trace.py:1116: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the repeated trace. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 1000 / 1000 (100.0%)\n",
      "Greatest absolute difference: 3.6893488147419103e+19 at index (0, 18) (up to 1e-05 allowed)\n",
      "Greatest relative difference: inf at index (0, 2) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trt_gm = torch_tensorrt.compile(model, inputs=inputs)\n",
    "torch_tensorrt.save(trt_gm, \"trt.ts\", output_format=\"torchscript\", inputs=inputs) # for C++ inference(C++ support only torchscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment in C++:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#include \"torch/script.h\"\n",
    "#include \"torch_tensorrt/torch_tensorrt.h\"\n",
    "\n",
    "# auto trt_mod = torch::jit::load(\"trt.ts\");\n",
    "# auto input_tensor = [...]; // fill this with your inputs\n",
    "# auto results = trt_mod.forward({input_tensor});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().cuda()\n",
    "inputs = [torch.randn((1, 3, 224, 224)).cuda()]\n",
    "# trt_ep is a torch.fx.GraphModule object\n",
    "trt_gm = torch_tensorrt.compile(model, ir=\"dynamo\", inputs=inputs)\n",
    "torch_tensorrt.save(trt_gm, \"trt.ep\", inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "model = torch.export.load(\"trt.ep\").module()\n",
    "model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
